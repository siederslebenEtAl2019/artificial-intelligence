{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/johsieders/neuronal-networks/blob/master/exploring/linear.ipynb\"\n",
    "target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch's Linear Layer Explained\n",
    "This lesson introduces PyTorch's Linear layer (`torch.nn.Linear`). To that end we look first at linear mappings in general and how they are dealt with in PyTorch. We therefore use PyTorch names and conventions. Dimensions (or shapes) are paramount when working with PyTorch (or any other library). A $(1 \\times n)$-Tensor is a row vector, $(n \\times 1)$-Tensor is a column vector,, and a $(n)$-vector can be used as either.\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Mappings\n",
    "\n",
    "A linear mapping is given by a $(q \\times p)$-matrix $W$ ($W$ for weight) and a $(1 \\times q)$-vector $b$ ($b$ for bias). It maps an $(n \\times p)$-matrice $X$ to an $(n \\times p)$-matrice $Y$. The number $n$ of rows can be any integer greater zero. The mapping is defined as\n",
    "\n",
    "> $ Y = X \\cdot W^{T} + B $\n",
    "\n",
    "> $ (n \\times q) \\cong (n \\times p) \\cdot (p \\times q) + (n \\times q)$\n",
    "\n",
    "with $B$ consisting of $n$ identical rows:\n",
    "\n",
    ">$B = \\left[ \\begin{array}{rrrr}\n",
    "b  \\\\\n",
    "\\vdots\\\\\n",
    "b  \\\\\n",
    "\\end{array}\\right] $\n",
    "\n",
    "So, the row vector $b$ is added to each row of $X\\cdot W^{T}$. Note that weight $W$ and bias $b$ keep their dimensions forever, whereas $X$ features an arbitrary number $n$ of rows. The rationale is as follows: $p$ is the number of features observed, $n$ the number of measurements, and $Y$ the expected outcome to be compared to some target $T$.  \n",
    "\n",
    "This is what it looks like in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Mappings\n",
    "\n",
    "A linear mapping is given by a $(q \\times p)$-matrix $W$ ($W$ for weight) and a $(1 \\times q)$-vector $b$ ($b$ for bias). It maps an $(n \\times p)$-matrice $X$ to an $(n \\times p)$-matrice $Y$. The number $n$ of rows can be any integer greater zero. The mapping is defined as\n",
    "\n",
    "> $ Y = X \\cdot W^{T} + B $\n",
    "\n",
    "> $ (n \\times q) \\cong (n \\times p) \\cdot (p \\times q) + (n \\times q)$\n",
    "\n",
    "with $B$ consisting of $n$ identical rows:\n",
    "\n",
    ">$B = \\left[ \\begin{array}{rrrr}\n",
    "b  \\\\\n",
    "\\vdots\\\\\n",
    "b  \\\\\n",
    "\\end{array}\\right] $\n",
    "\n",
    "So, the row vector $b$ is added to each row of $X\\cdot W^{T}$. Note that weight $W$ and bias $b$ keep their dimensions forever, whereas $X$ features an arbitrary number $n$ of rows. The rationale is as follows: $p$ is the number of features observed, $n$ the number of measurements, and $Y$ the expected outcome to be compared to some target $T$.  \n",
    "\n",
    "This is what it looks like in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[26., 26., 26.],\n",
      "        [46., 46., 46.],\n",
      "        [66., 66., 66.],\n",
      "        [86., 86., 86.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def exploreBasics():\n",
    " \n",
    "  n = 4     # number of measurements\n",
    "  p = 2     # number of input features\n",
    "  q = 3     # number of output features\n",
    "\n",
    "  b_shape = [q]\n",
    "  w_shape = [q, p]\n",
    "  x_shape = [n, p]\n",
    "\n",
    "  b = torch.tensor([3., 3., 3.])  # bias\n",
    "  b = b.view(b_shape)\n",
    "\n",
    "  w = torch.tensor([1., 1., 1., 1., 1., 1.])  # weight\n",
    "  w = w.view(w_shape) \n",
    "\n",
    "  x = torch.tensor([11., 12., 21., 22., 31., 32., 41., 42.])  # features\n",
    "  x = x.view(x_shape) \n",
    "\n",
    "  # mm stands for matrix multiplication\n",
    "  # b is automatically added to all rows \n",
    "\n",
    "  y = x.mm(w.t()) + b\n",
    "  print(y)\n",
    "\n",
    "exploreBasics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Layer\n",
    "\n",
    "PyTorch features numerous layers each of which wraps a particular functions. So, `Identity` wraps the identity function, `Linear` wraps linear mappings, `Conv1d` wraps one dimensional convolutions and so on. What do these wrappings do? Let us look at `Linear`, an easy one.\n",
    "\n",
    "Each object `lin` of `Linear` is a function object which applies a linear mapping defined by weight and bias to the argument x. Weight and bias are stored in the static dictionary of `lin`, accessible by name (`weight`and `bias`). The use of Linear and any other layer is at least twofold:\n",
    "* In the course of an optimization (e.g. steepest gradient descent), the function object is repeatedly called, with weight and bias being adjusted at each step .\n",
    "* Linear and all other layers play a crucial role in the backpropagation algorithm. This is part of tutorial on its own << todo >> \n",
    "\n",
    "The `Linear` constructor takes two parameters: `p`, the number of input features and `q`, the number of output features. At construction, weight and bias are randomly initialized. These parameters can be accessed by `state_dict()`, set to any values and reloaded by means of `load_state_dict()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def exploreApply():\n",
    "\n",
    "  n = 4     # number of measurements\n",
    "  p = 2     # number of input features\n",
    "  q = 3     # number of output features\n",
    "\n",
    "  l_shape = [p, q]\n",
    "  x_shape = [n, p]\n",
    "\n",
    "  # lin is a function object as described above\n",
    "  lin = nn.Linear(*l_shape)     # weight : p x q, bias : q\n",
    "\n",
    "  # getting weight and bias from the state dictionary\n",
    "  dict = lin.state_dict()\n",
    "  w = dict['weight']\n",
    "  b = dict['bias']\n",
    "\n",
    "  x = torch.tensor([11., 12., 21., 22., 31., 32., 41., 42.])  # features\n",
    "  x = x.view(x_shape) \n",
    "\n",
    "  # applying lin = Linear(2, 3) and checking the result\n",
    "  y = lin(x)\n",
    "\n",
    "  # this is what lin does\n",
    "  z = x.mm(w.t()) + b  # (n x p) * (p x q)  + (n x q)\n",
    "\n",
    "  if not torch.equal(y, z):\n",
    "    raise Exception\n",
    "\n",
    "  print('run successfully')\n",
    "\n",
    "exploreApply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-d351889f1637>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-3-d351889f1637>\"\u001B[1;36m, line \u001B[1;32m2\u001B[0m\n\u001B[1;33m    lin = nn.Linear(2, 3)     # weight : 3 x 2, bias : 3\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "  # lin is a function object as described above\n",
    "      lin = nn.Linear(2, 3)     # weight : 3 x 2, bias : 3\n",
    "\n",
    "      # x is an input vector with two features (per row) four times measured\n",
    "      x = torch.tensor([5., 6., 30., 40., 200., 600., 0., 0],\n",
    "                      requires_grad=True).reshape(x_shape)  # x : 4 x 2\n",
    "\n",
    "      # setting weight and bias to arbitrary values\n",
    "      w = torch.full(w_shape, 2.)\n",
    "      b = torch.full(b_shape, 7.)\n",
    "      xd = {'weight': w, 'bias': b}\n",
    "      lin.load_state_dict(xd)\n",
    "\n",
    "      # applying lin = Linear(2, 3) and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b  # (3 x 2) * (2 x 1)  + (3 x 1)\n",
    "      if  not torch.equal(y, z):\n",
    "        raise Exception\n",
    "\n",
    "      # sum is an arbitrary scalar-valued function\n",
    "      s = y.sum()\n",
    "\n",
    "      # computing the gradients for weight and bias with respect to s\n",
    "      s.backward()\n",
    "\n",
    "      # one step of gradient descent applied to weight and bias\n",
    "      for p in lin.parameters():\n",
    "          p.data -= p.grad.data\n",
    "\n",
    "      # getting weight and bias from the state dictionary\n",
    "      xd = lin.state_dict()\n",
    "      w = xd['weight']\n",
    "      b = xd['bias']\n",
    "\n",
    "      # applying lin = Linear(2, 3) with modified parameters and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b\n",
    "      if  not torch.equal(y, z):\n",
    "          raise Exception\n",
    "      \n",
    "      print('run successfully')\n",
    "\n",
    "testLinear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # lin is a function object as described above\n",
    "      lin = nn.Linear(2, 3)     # weight : 3 x 2, bias : 3\n",
    "\n",
    "      # x is an input vector with two features (per row) four times measured\n",
    "      x = torch.tensor([5., 6., 30., 40., 200., 600., 0., 0],\n",
    "                      requires_grad=True).reshape(x_shape)  # x : 4 x 2\n",
    "\n",
    "      # setting weight and bias to arbitrary values\n",
    "      w = torch.full(w_shape, 2.)\n",
    "      b = torch.full(b_shape, 7.)\n",
    "      xd = {'weight': w, 'bias': b}\n",
    "      lin.load_state_dict(xd)\n",
    "\n",
    "      # applying lin = Linear(2, 3) and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b  # (3 x 2) * (2 x 1)  + (3 x 1)\n",
    "      if  not torch.equal(y, z):\n",
    "        raise Exception\n",
    "\n",
    "      # sum is an arbitrary scalar-valued function\n",
    "      s = y.sum()\n",
    "\n",
    "      # computing the gradients for weight and bias with respect to s\n",
    "      s.backward()\n",
    "\n",
    "      # one step of gradient descent applied to weight and bias\n",
    "      for p in lin.parameters():\n",
    "          p.data -= p.grad.data\n",
    "\n",
    "      # getting weight and bias from the state dictionary\n",
    "      xd = lin.state_dict()\n",
    "      w = xd['weight']\n",
    "      b = xd['bias']\n",
    "\n",
    "      # applying lin = Linear(2, 3) with modified parameters and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b\n",
    "      if  not torch.equal(y, z):\n",
    "          raise Exception\n",
    "      \n",
    "      print('run successfully')\n",
    "\n",
    "testLinear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SgQwL20GkW-Z"
   },
   "source": [
    "## Linear Mappings\n",
    "\n",
    "A linear mapping is given by a $(q \\times p)$-matrix $W$ ($W$ for weight) and a $(1 \\times q)$-vector $b$ ($b$ for bias). It maps an $(n \\times p)$-matrice $X$ to an $(n \\times p)$-matrice $Y$. The number $n$ of rows can be any integer greater zero. The mapping is defined as\n",
    "\n",
    "> $ Y = X \\cdot W^{T} + B $\n",
    "\n",
    "> $ (n \\times q) \\cong (n \\times p) \\cdot (p \\times q) + (n \\times q)$\n",
    "\n",
    "with $B$ consisting of $n$ identical rows:\n",
    "\n",
    ">$B = \\left[ \\begin{array}{rrrr}\n",
    "b  \\\\\n",
    "\\vdots\\\\\n",
    "b  \\\\\n",
    "\\end{array}\\right] $\n",
    "\n",
    "So, the row vector $b$ is added to each row of $X\\cdot W^{T}$. Note that weight $W$ and bias $b$ keep their dimensions forever, whereas $X$ features an arbitrary number $n$ of rows. The rationale is as follows: $p$ is the number of features observed, $n$ the number of measurements, and $Y$ the expected outcome to be compared to some target $T$.  \n",
    "\n",
    "This is what it looks like in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_tKOOCu9H8GN"
   },
   "source": [
    "## Linear Mappings\n",
    "\n",
    "A linear mapping is given by a $(q \\times p)$-matrix $W$ ($W$ for weight) and a $(1 \\times q)$-vector $b$ ($b$ for bias). It maps an $(n \\times p)$-matrice $X$ to an $(n \\times p)$-matrice $Y$. The number $n$ of rows can be any integer greater zero. The mapping is defined as\n",
    "\n",
    "> $ Y = X \\cdot W^{T} + B $\n",
    "\n",
    "> $ (n \\times q) \\cong (n \\times p) \\cdot (p \\times q) + (n \\times q)$\n",
    "\n",
    "with $B$ consisting of $n$ identical rows:\n",
    "\n",
    ">$B = \\left[ \\begin{array}{rrrr}\n",
    "b  \\\\\n",
    "\\vdots\\\\\n",
    "b  \\\\\n",
    "\\end{array}\\right] $\n",
    "\n",
    "So, the row vector $b$ is added to each row of $X\\cdot W^{T}$. Note that weight $W$ and bias $b$ keep their dimensions forever, whereas $X$ features an arbitrary number $n$ of rows. The rationale is as follows: $p$ is the number of features observed, $n$ the number of measurements, and $Y$ the expected outcome to be compared to some target $T$.  \n",
    "\n",
    "This is what it looks like in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoWd5z3ORBHf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def exploreBasics():\n",
    " \n",
    "  n = 4     # number of measurements\n",
    "  p = 2     # number of input features\n",
    "  q = 3     # number of output features\n",
    "\n",
    "  b_shape = [q]\n",
    "  w_shape = [q, p]\n",
    "  x_shape = [n, p]\n",
    "\n",
    "  b = torch.tensor([3., 3., 3.])  # bias\n",
    "  b = b.view(b_shape)\n",
    "\n",
    "  w = torch.tensor([1., 1., 1., 1., 1., 1.])  # weight\n",
    "  w = w.view(w_shape) \n",
    "\n",
    "  x = torch.tensor([11., 12., 21., 22., 31., 32., 41., 42.])  # features\n",
    "  x = x.view(x_shape) \n",
    "\n",
    "  # mm stands for matrix multiplication\n",
    "  # b is automatically added to all rows \n",
    "\n",
    "  y = x.mm(w.t()) + b\n",
    "  print(y)\n",
    "\n",
    "exploreBasics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uyflq0YLXQgX"
   },
   "source": [
    "## The Linear Layer\n",
    "\n",
    "PyTorch features numerous layers each of which wraps a particular functions. So, `Identity` wraps the identity function, `Linear` wraps linear mappings, `Conv1d` wraps one dimensional convolutions and so on. What do these wrappings do? Let us look at `Linear`, an easy one.\n",
    "\n",
    "Each object `lin` of `Linear` is a function object which applies a linear mapping defined by weight and bias to the argument x. Weight and bias are stored in the static dictionary of `lin`, accessible by name (`weight`and `bias`). The use of Linear and any other layer is at least twofold:\n",
    "* In the course of an optimization (e.g. steepest gradient descent), the function object is repeatedly called, with weight and bias being adjusted at each step .\n",
    "* Linear and all other layers play a crucial role in the backpropagation algorithm. This is part of tutorial on its own << todo >> \n",
    "\n",
    "The `Linear` constructor takes two parameters: `p`, the number of input features and `q`, the number of output features. At construction, weight and bias are randomly initialized. These parameters can be accessed by `state_dict()`, set to any values and reloaded by means of `load_state_dict()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yW89AIq8Yotx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def exploreApply():\n",
    "\n",
    "  n = 4     # number of measurements\n",
    "  p = 2     # number of input features\n",
    "  q = 3     # number of output features\n",
    "\n",
    "  l_shape = [p, q]\n",
    "  x_shape = [n, p]\n",
    "\n",
    "  # lin is a function object as described above\n",
    "  lin = nn.Linear(*l_shape)     # weight : p x q, bias : q\n",
    "\n",
    "  # getting weight and bias from the state dictionary\n",
    "  dict = lin.state_dict()\n",
    "  w = dict['weight']\n",
    "  b = dict['bias']\n",
    "\n",
    "  x = torch.tensor([11., 12., 21., 22., 31., 32., 41., 42.])  # features\n",
    "  x = x.view(x_shape) \n",
    "\n",
    "  # applying lin = Linear(2, 3) and checking the result\n",
    "  y = lin(x)\n",
    "\n",
    "  # this is what lin does\n",
    "  z = x.mm(w.t()) + b  # (n x p) * (p x q)  + (n x q)\n",
    "\n",
    "  if not torch.equal(y, z):\n",
    "    raise Exception\n",
    "\n",
    "  print('run successfully')\n",
    "\n",
    "exploreApply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkGEUV0wftXe"
   },
   "outputs": [],
   "source": [
    "  # lin is a function object as described above\n",
    "      lin = nn.Linear(2, 3)     # weight : 3 x 2, bias : 3\n",
    "\n",
    "      # x is an input vector with two features (per row) four times measured\n",
    "      x = torch.tensor([5., 6., 30., 40., 200., 600., 0., 0],\n",
    "                      requires_grad=True).reshape(x_shape)  # x : 4 x 2\n",
    "\n",
    "      # setting weight and bias to arbitrary values\n",
    "      w = torch.full(w_shape, 2.)\n",
    "      b = torch.full(b_shape, 7.)\n",
    "      xd = {'weight': w, 'bias': b}\n",
    "      lin.load_state_dict(xd)\n",
    "\n",
    "      # applying lin = Linear(2, 3) and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b  # (3 x 2) * (2 x 1)  + (3 x 1)\n",
    "      if  not torch.equal(y, z):\n",
    "        raise Exception\n",
    "\n",
    "      # sum is an arbitrary scalar-valued function\n",
    "      s = y.sum()\n",
    "\n",
    "      # computing the gradients for weight and bias with respect to s\n",
    "      s.backward()\n",
    "\n",
    "      # one step of gradient descent applied to weight and bias\n",
    "      for p in lin.parameters():\n",
    "          p.data -= p.grad.data\n",
    "\n",
    "      # getting weight and bias from the state dictionary\n",
    "      xd = lin.state_dict()\n",
    "      w = xd['weight']\n",
    "      b = xd['bias']\n",
    "\n",
    "      # applying lin = Linear(2, 3) with modified parameters and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b\n",
    "      if  not torch.equal(y, z):\n",
    "          raise Exception\n",
    "      \n",
    "      print('run successfully')\n",
    "\n",
    "testLinear()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP0+w2esjbNFiN9sGGSyJyR",
   "include_colab_link": true,
   "mount_file_id": "1YN8ESM2u2q6r_dWyrJl0HwNLpimMXqgM",
   "name": "linear.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "pycharm-16c786a7",
   "language": "python",
   "display_name": "PyCharm (neuronal-networks)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}