{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/johsieders/artificial-intelligence/blob/master/exploring/autograd.ipynb\"\n",
    "target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch's Autograd Explained\n",
    "This lesson introduces PyTorch's autograd library. The feature autograd relies basically on the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients along Different Paths are Added\n",
    "\n",
    "\n",
    "Let us look a a simple example.\n",
    "\n",
    "![an example](autograd.png)\n",
    "\n",
    "So, the row vector $b$ is added to each row of $X\\cdot W^{T}$. Note that weight $W$ and bias $b$ keep\n",
    "their dimensions forever, whereas $X$ features an arbitrary number $n$ of rows. The rationale is as follows:\n",
    "$p$ is the number of features observed, $n$ the number of measurements, and $Y$ the expected outcome\n",
    "to be compared to some target $T$.\n",
    "\n",
    "![Image of Yaktocat](https://octodex.github.com/images/yaktocat.png)\n",
    "\n",
    "<img src=\"autograd.png\",width=60,height=60>\n",
    "\n",
    "This is what it looks like in PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": "<img src=\"https://github.com/johsieders/artificial-intelligence/src/exploring/autograd.png\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System:\n",
      "    python: 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]\n",
      "executable: C:\\Users\\j.siedersleben\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n",
      "   machine: Windows-10-10.0.18362-SP0\n",
      "\n",
      "Python dependencies:\n",
      "       pip: 20.0.2\n",
      "setuptools: 46.2.0.post20200511\n",
      "   sklearn: 0.22.1\n",
      "     numpy: 1.18.1\n",
      "     scipy: 1.4.1\n",
      "    Cython: None\n",
      "    pandas: None\n",
      "matplotlib: 3.2.1\n",
      "    joblib: 0.14.1\n",
      "\n",
      "Built with OpenMP: True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Image, SVG, Math, YouTubeVideo\n",
    "\n",
    "i = Image(url='http://jupyter.org/assets/nav_logo.svg')\n",
    "\n",
    "j = Image(url='https://github.com/johsieders/artificial-intelligence/src/exploring/autograd.png')\n",
    "\n",
    "h = Image(filename='autograd.png')\n",
    "\n",
    "# display(i)\n",
    "display(j)\n",
    "# display(h)\n",
    "\n",
    "\n",
    "import sklearn\n",
    "s = sklearn.show_versions()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%md\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run yy successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def testScalar():\n",
    "        \"\"\"\n",
    "        scalar --> scalar --> scalar\n",
    "        This shows:\n",
    "        * gradients along different paths are added\n",
    "        * each forward step allows one backward step.\n",
    "\n",
    "        \"\"\"\n",
    "        x = torch.tensor(1., requires_grad=True)\n",
    "\n",
    "        s = x ** 2\n",
    "        t = x ** 3\n",
    "        y = s * t  # x ** 5 == 1\n",
    "\n",
    "        y.backward()\n",
    "        if x.grad != 5:  # 5 = 5 * x ** 4\n",
    "            raise Exception\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x += 1\n",
    "            x.grad.zero_()   # is mandatory\n",
    "\n",
    "        s = x ** 2\n",
    "        t = x ** 3\n",
    "        y = s * t  # x ** 5\n",
    "\n",
    "        y.backward()\n",
    "        if x.grad != 80:  # 80 = 5 * x ** 4\n",
    "            raise Exception\n",
    "\n",
    "        print('run yy successfully')\n",
    "\n",
    "testScalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Layer\n",
    "\n",
    "\n",
    "\n",
    "The `Linear` constructor takes two parameters: `p`, the number of input features and `q`, the number of output features. At construction, weight and bias are randomly initialized. These parameters can be accessed by `state_dict()`, set to any values and reloaded by means of `load_state_dict()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def exploreApply():\n",
    "\n",
    "  n = 4     # number of measurements\n",
    "  p = 2     # number of input features\n",
    "  q = 3     # number of output features\n",
    "\n",
    "  l_shape = [p, q]\n",
    "  x_shape = [n, p]\n",
    "\n",
    "  # lin is a function object as described above\n",
    "  lin = nn.Linear(*l_shape)     # weight : p x q, bias : q\n",
    "\n",
    "  # getting weight and bias from the state dictionary\n",
    "  dict = lin.state_dict()\n",
    "  w = dict['weight']\n",
    "  b = dict['bias']\n",
    "\n",
    "  x = torch.tensor([11., 12., 21., 22., 31., 32., 41., 42.])  # features\n",
    "  x = x.view(x_shape) \n",
    "\n",
    "  # applying lin = Linear(2, 3) and checking the result\n",
    "  y = lin(x)\n",
    "\n",
    "  # this is what lin does\n",
    "  z = x.mm(w.t()) + b  # (n x p) * (p x q)  + (n x q)\n",
    "\n",
    "  if not torch.equal(y, z):\n",
    "    raise Exception\n",
    "\n",
    "  print('run successfully')\n",
    "\n",
    "exploreApply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-d351889f1637>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-3-d351889f1637>\"\u001B[1;36m, line \u001B[1;32m2\u001B[0m\n\u001B[1;33m    lin = nn.Linear(2, 3)     # weight : 3 x 2, bias : 3\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "  # lin is a function object as described above\n",
    "      lin = nn.Linear(2, 3)     # weight : 3 x 2, bias : 3\n",
    "\n",
    "      # x is an input vector with two features (per row) four times measured\n",
    "      x = torch.tensor([5., 6., 30., 40., 200., 600., 0., 0],\n",
    "                      requires_grad=True).reshape(x_shape)  # x : 4 x 2\n",
    "\n",
    "      # setting weight and bias to arbitrary values\n",
    "      w = torch.full(w_shape, 2.)\n",
    "      b = torch.full(b_shape, 7.)\n",
    "      xd = {'weight': w, 'bias': b}\n",
    "      lin.load_state_dict(xd)\n",
    "\n",
    "      # applying lin = Linear(2, 3) and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b  # (3 x 2) * (2 x 1)  + (3 x 1)\n",
    "      if  not torch.equal(y, z):\n",
    "        raise Exception\n",
    "\n",
    "      # sum is an arbitrary scalar-valued function\n",
    "      s = y.sum()\n",
    "\n",
    "      # computing the gradients for weight and bias with respect to s\n",
    "      s.backward()\n",
    "\n",
    "      # one step of gradient descent applied to weight and bias\n",
    "      for p in lin.parameters():\n",
    "          p.data -= p.grad.data\n",
    "\n",
    "      # getting weight and bias from the state dictionary\n",
    "      xd = lin.state_dict()\n",
    "      w = xd['weight']\n",
    "      b = xd['bias']\n",
    "\n",
    "      # applying lin = Linear(2, 3) with modified parameters and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b\n",
    "      if  not torch.equal(y, z):\n",
    "          raise Exception\n",
    "      \n",
    "      print('run successfully')\n",
    "\n",
    "testLinear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # lin is a function object as described above\n",
    "      lin = nn.Linear(2, 3)     # weight : 3 x 2, bias : 3\n",
    "\n",
    "      # x is an input vector with two features (per row) four times measured\n",
    "      x = torch.tensor([5., 6., 30., 40., 200., 600., 0., 0],\n",
    "                      requires_grad=True).reshape(x_shape)  # x : 4 x 2\n",
    "\n",
    "      # setting weight and bias to arbitrary values\n",
    "      w = torch.full(w_shape, 2.)\n",
    "      b = torch.full(b_shape, 7.)\n",
    "      xd = {'weight': w, 'bias': b}\n",
    "      lin.load_state_dict(xd)\n",
    "\n",
    "      # applying lin = Linear(2, 3) and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b  # (3 x 2) * (2 x 1)  + (3 x 1)\n",
    "      if  not torch.equal(y, z):\n",
    "        raise Exception\n",
    "\n",
    "      # sum is an arbitrary scalar-valued function\n",
    "      s = y.sum()\n",
    "\n",
    "      # computing the gradients for weight and bias with respect to s\n",
    "      s.backward()\n",
    "\n",
    "      # one step of gradient descent applied to weight and bias\n",
    "      for p in lin.parameters():\n",
    "          p.data -= p.grad.data\n",
    "\n",
    "      # getting weight and bias from the state dictionary\n",
    "      xd = lin.state_dict()\n",
    "      w = xd['weight']\n",
    "      b = xd['bias']\n",
    "\n",
    "      # applying lin = Linear(2, 3) with modified parameters and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b\n",
    "      if  not torch.equal(y, z):\n",
    "          raise Exception\n",
    "      \n",
    "      print('run successfully')\n",
    "\n",
    "testLinear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SgQwL20GkW-Z"
   },
   "source": [
    "## Linear Mappings\n",
    "\n",
    "A linear mapping is given by a $(q \\times p)$-matrix $W$ ($W$ for weight) and a $(1 \\times q)$-vector $b$ ($b$ for bias). It maps an $(n \\times p)$-matrice $X$ to an $(n \\times p)$-matrice $Y$. The number $n$ of rows can be any integer greater zero. The mapping is defined as\n",
    "\n",
    "> $ Y = X \\cdot W^{T} + B $\n",
    "\n",
    "> $ (n \\times q) \\cong (n \\times p) \\cdot (p \\times q) + (n \\times q)$\n",
    "\n",
    "with $B$ consisting of $n$ identical rows:\n",
    "\n",
    ">$B = \\left[ \\begin{array}{rrrr}\n",
    "b  \\\\\n",
    "\\vdots\\\\\n",
    "b  \\\\\n",
    "\\end{array}\\right] $\n",
    "\n",
    "So, the row vector $b$ is added to each row of $X\\cdot W^{T}$. Note that weight $W$ and bias $b$ keep their dimensions forever, whereas $X$ features an arbitrary number $n$ of rows. The rationale is as follows: $p$ is the number of features observed, $n$ the number of measurements, and $Y$ the expected outcome to be compared to some target $T$.  \n",
    "\n",
    "This is what it looks like in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_tKOOCu9H8GN"
   },
   "source": [
    "## Linear Mappings\n",
    "\n",
    "A linear mapping is given by a $(q \\times p)$-matrix $W$ ($W$ for weight) and a $(1 \\times q)$-vector $b$ ($b$ for bias). It maps an $(n \\times p)$-matrice $X$ to an $(n \\times p)$-matrice $Y$. The number $n$ of rows can be any integer greater zero. The mapping is defined as\n",
    "\n",
    "> $ Y = X \\cdot W^{T} + B $\n",
    "\n",
    "> $ (n \\times q) \\cong (n \\times p) \\cdot (p \\times q) + (n \\times q)$\n",
    "\n",
    "with $B$ consisting of $n$ identical rows:\n",
    "\n",
    ">$B = \\left[ \\begin{array}{rrrr}\n",
    "b  \\\\\n",
    "\\vdots\\\\\n",
    "b  \\\\\n",
    "\\end{array}\\right] $\n",
    "\n",
    "So, the row vector $b$ is added to each row of $X\\cdot W^{T}$. Note that weight $W$ and bias $b$ keep their dimensions forever, whereas $X$ features an arbitrary number $n$ of rows. The rationale is as follows: $p$ is the number of features observed, $n$ the number of measurements, and $Y$ the expected outcome to be compared to some target $T$.  \n",
    "\n",
    "This is what it looks like in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoWd5z3ORBHf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def exploreBasics():\n",
    " \n",
    "  n = 4     # number of measurements\n",
    "  p = 2     # number of input features\n",
    "  q = 3     # number of output features\n",
    "\n",
    "  b_shape = [q]\n",
    "  w_shape = [q, p]\n",
    "  x_shape = [n, p]\n",
    "\n",
    "  b = torch.tensor([3., 3., 3.])  # bias\n",
    "  b = b.view(b_shape)\n",
    "\n",
    "  w = torch.tensor([1., 1., 1., 1., 1., 1.])  # weight\n",
    "  w = w.view(w_shape) \n",
    "\n",
    "  x = torch.tensor([11., 12., 21., 22., 31., 32., 41., 42.])  # features\n",
    "  x = x.view(x_shape) \n",
    "\n",
    "  # mm stands for matrix multiplication\n",
    "  # b is automatically added to all rows \n",
    "\n",
    "  y = x.mm(w.t()) + b\n",
    "  print(y)\n",
    "\n",
    "exploreBasics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uyflq0YLXQgX"
   },
   "source": [
    "## The Linear Layer\n",
    "\n",
    "PyTorch features numerous layers each of which wraps a particular functions. So, `Identity` wraps the identity function, `Linear` wraps linear mappings, `Conv1d` wraps one dimensional convolutions and so on. What do these wrappings do? Let us look at `Linear`, an easy one.\n",
    "\n",
    "Each object `lin` of `Linear` is a function object which applies a linear mapping defined by weight and bias to the argument x. Weight and bias are stored in the static dictionary of `lin`, accessible by name (`weight`and `bias`). The use of Linear and any other layer is at least twofold:\n",
    "* In the course of an optimization (e.g. steepest gradient descent), the function object is repeatedly called, with weight and bias being adjusted at each step .\n",
    "* Linear and all other layers play a crucial role in the backpropagation algorithm. This is part of tutorial on its own << todo >> \n",
    "\n",
    "The `Linear` constructor takes two parameters: `p`, the number of input features and `q`, the number of output features. At construction, weight and bias are randomly initialized. These parameters can be accessed by `state_dict()`, set to any values and reloaded by means of `load_state_dict()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yW89AIq8Yotx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def exploreApply():\n",
    "\n",
    "  n = 4     # number of measurements\n",
    "  p = 2     # number of input features\n",
    "  q = 3     # number of output features\n",
    "\n",
    "  l_shape = [p, q]\n",
    "  x_shape = [n, p]\n",
    "\n",
    "  # lin is a function object as described above\n",
    "  lin = nn.Linear(*l_shape)     # weight : p x q, bias : q\n",
    "\n",
    "  # getting weight and bias from the state dictionary\n",
    "  dict = lin.state_dict()\n",
    "  w = dict['weight']\n",
    "  b = dict['bias']\n",
    "\n",
    "  x = torch.tensor([11., 12., 21., 22., 31., 32., 41., 42.])  # features\n",
    "  x = x.view(x_shape) \n",
    "\n",
    "  # applying lin = Linear(2, 3) and checking the result\n",
    "  y = lin(x)\n",
    "\n",
    "  # this is what lin does\n",
    "  z = x.mm(w.t()) + b  # (n x p) * (p x q)  + (n x q)\n",
    "\n",
    "  if not torch.equal(y, z):\n",
    "    raise Exception\n",
    "\n",
    "  print('run successfully')\n",
    "\n",
    "exploreApply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkGEUV0wftXe"
   },
   "outputs": [],
   "source": [
    "  # lin is a function object as described above\n",
    "      lin = nn.Linear(2, 3)     # weight : 3 x 2, bias : 3\n",
    "\n",
    "      # x is an input vector with two features (per row) four times measured\n",
    "      x = torch.tensor([5., 6., 30., 40., 200., 600., 0., 0],\n",
    "                      requires_grad=True).reshape(x_shape)  # x : 4 x 2\n",
    "\n",
    "      # setting weight and bias to arbitrary values\n",
    "      w = torch.full(w_shape, 2.)\n",
    "      b = torch.full(b_shape, 7.)\n",
    "      xd = {'weight': w, 'bias': b}\n",
    "      lin.load_state_dict(xd)\n",
    "\n",
    "      # applying lin = Linear(2, 3) and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b  # (3 x 2) * (2 x 1)  + (3 x 1)\n",
    "      if  not torch.equal(y, z):\n",
    "        raise Exception\n",
    "\n",
    "      # sum is an arbitrary scalar-valued function\n",
    "      s = y.sum()\n",
    "\n",
    "      # computing the gradients for weight and bias with respect to s\n",
    "      s.backward()\n",
    "\n",
    "      # one step of gradient descent applied to weight and bias\n",
    "      for p in lin.parameters():\n",
    "          p.data -= p.grad.data\n",
    "\n",
    "      # getting weight and bias from the state dictionary\n",
    "      xd = lin.state_dict()\n",
    "      w = xd['weight']\n",
    "      b = xd['bias']\n",
    "\n",
    "      # applying lin = Linear(2, 3) with modified parameters and checking the result\n",
    "      y = lin(x)\n",
    "      z = x.mm(w.t()) + b\n",
    "      if  not torch.equal(y, z):\n",
    "          raise Exception\n",
    "      \n",
    "      print('run successfully')\n",
    "\n",
    "testLinear()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP0+w2esjbNFiN9sGGSyJyR",
   "include_colab_link": true,
   "mount_file_id": "1YN8ESM2u2q6r_dWyrJl0HwNLpimMXqgM",
   "name": "linear.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "pycharm-16c786a7",
   "language": "python",
   "display_name": "PyCharm (neuronal-networks)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}