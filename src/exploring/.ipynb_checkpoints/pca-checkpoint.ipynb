{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PCA Explained\n",
    "\n",
    "\n",
    "\n",
    "PCA stands for Principal Component Analysis. The question is: Out of many features, how can\n",
    "how can I find the relevant ones and discard the others?\n",
    "\n",
    "Let $X (n \\times p)$ be the input matrix with $n$ samples and $p$ features. We would like\n",
    "to figure out what the $r$ most important features are.\n",
    "\n",
    "Todo: We are talking about linear combinations of the features given. The most important features are\n",
    "the eigenvectors of $X^TX$ in descending order of the respective eigenvalues.\n",
    "Explainingg how covariance comes in.\n",
    "\n",
    "To this end, we proceed as follows:\n",
    "\n",
    "1. We normalize $X$ to its mean, that is, from each column of $X$ we subtract its mean. Recall that a\n",
    "the $i$-th column of $X$ represents $n$ samples of feature $i$\n",
    "2. We compute the matrix $\\Lambda$ of the eigenvalues of $X^{T}X$. THis matrix is symmetric and\n",
    "therefore diagonalisable.\n",
    "\n",
    ">$\\Lambda =\n",
    "  \\begin{bmatrix}\n",
    "    \\lambda_{1} & & \\\\\n",
    "    & \\ddots & \\\\\n",
    "    & & \\lambda_{p}\n",
    "  \\end{bmatrix}$\n",
    "\n",
    "$\\Lambda$ is obtained as follows:\n",
    "The first eigenvalue $v_{1}$ is given by\n",
    "\n",
    "> $v_{1} = \\underset{\\|{v}\\| = 1}{arg max} \\{v^{T}X^{T}Xv\\}$\n",
    "\n",
    "This gives us the eigenvector $v_{1}$ with the largest eigenvalue $\\lambda_1$\n",
    "Setting $X_0 = X$ and\n",
    "\n",
    "> $X_1 = X_0 - X_0 v_1 v_1^T \\qquad (n \\times p)(p \\times 1)(1 \\times p) \\cong (n \\times p)$\n",
    "\n",
    "we obtain\n",
    "\n",
    "> $v_{2} = \\underset{\\|{v}\\| = 1}{arg max} \\{v^{T}X_1^{T}X_1v\\}$\n",
    "\n",
    "Continuing this procedure until $v_{2}$ we obtain the $p \\times p$ matrix $V = [v_1, \\dots, v_p]$\n",
    "$V$ is a basis transformation of $\\mathbb{R}^p$; $Z = XV$ represents $X$ with respect to the new basis,\n",
    "in which the most important features come first. So, if we want to keep the first $R$ features and discard\n",
    "the rest, we set\n",
    "\n",
    "> $Z_r = XV_r \\qquad (n \\times p) (p \\times r)$\n",
    "\n",
    "where $V_r$ represents the first $r$ column vectors of $V$. So, $p$ dimensions have been reduced to $r$,\n",
    "and we now solve\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[2., 0.],\n",
      "        [0., 1.]])\n",
      "tensor([[0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 2., 0.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# svd = singular value decomposition\n",
    "\n",
    "x = torch.tensor((1, 0, 0, 0, 1, 0),  dtype=torch.float).view(2, 3)\n",
    "\n",
    "u, s, v = torch.svd(x)\n",
    "s = torch.diag(s)\n",
    "y = u.mm(s).mm(v.t())\n",
    "\n",
    "\n",
    "print(u)\n",
    "print(s)\n",
    "print(v)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " ![Google's logo](https://www.google.com/images/logos/google_logo_41.png)\n",
    "\n",
    " ![autograd](autograd.png)\n",
    "\n",
    " ![autograd](autograd.ipynb)\n",
    " \n",
    " ![autograd](autograd.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">\\[a link](https://github.com/johsieders/artificial-intelligence/blob/master/src/exploring/autograd.ipynb)\n",
    ">\n",
    "<a href=\"https://github.com/johsieders/artificial-intelligence/blob/master/src/exploring/autograd.ipynb\">autograd</a>\n",
    "\n",
    "<a href=\"file:///C:\\Users\\j.siedersleben\\Google Drive\\PyCharmProjects\\artificial-intelligence\\src\\exploring\\autograd.ipynb\">autograd</a>\n",
    "\n",
    "<a href=\"file:///C:/Users/j.siedersleben/Google Drive/PyCharmProjects/artificial-intelligence/src/exploring/autograd.ipynb\">autograd</a>\n",
    ">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'linear_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ebeb40851df0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\PyCharmProjects\\artificial-intelligence\\src\\exploring\\sklearn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'linear_model'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (neuronal-networks)",
   "language": "python",
   "name": "pycharm-16c786a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
